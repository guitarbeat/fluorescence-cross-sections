"""
    --- AUTO-GENERATED DOCSTRING ---
    Table of content is automatically generated by Agent Docstrings v1.3.5

    Classes/Functions:
        - load_water_absorption_data() -> pd.DataFrame (line 40)
        - load_fluorophore_data() -> pd.DataFrame (line 69)
        - load_cross_section_data() -> Dict[str, pd.DataFrame] (line 85)
        
    --- END AUTO-GENERATED DOCSTRING ---

Data loading utilities for handling various data sources including:
- Water absorption data
- Fluorophore data
- Two-photon cross-section data
"""
import logging
from typing import Dict, Optional

import numpy as np
import pandas as pd
import streamlit as st

from src.config import DATA_DIR, FLUOROPHORE_COLUMNS, FLUOROPHORE_CSV

logger = logging.getLogger(__name__)

# Constants
XSECTION_DIR = DATA_DIR / "2p-xsections"
DEFAULT_COLUMNS = FLUOROPHORE_COLUMNS



def load_water_absorption_data() -> pd.DataFrame:
    """
    Load water absorption data from kou93b.dat.

    Returns:
        pd.DataFrame: DataFrame containing wavelength and absorption data
    """
    data_path = DATA_DIR / "kou93b.dat"
    try:
        df = pd.read_csv(
            data_path,
            sep=r'\s+',
            skiprows=6,
            names=["wavelength", "absorption"],
            encoding="latin1",
            comment="#",
        )
        # Flip the data as in the MATLAB code
        df = df.iloc[::-1].reset_index(drop=True)
        return df
    except (FileNotFoundError, pd.errors.EmptyDataError) as e:
        st.error(f"Error loading water absorption data: {e}")
        return pd.DataFrame({
            "wavelength": np.linspace(800, 2400, 1000),
            "absorption": np.zeros(1000),
        })



def load_fluorophore_data() -> pd.DataFrame:
    """
    Load existing fluorophores from CSV or create empty DataFrame.

    Returns:
        pd.DataFrame: DataFrame containing fluorophore data
    """
    try:
        return pd.read_csv(FLUOROPHORE_CSV)
    except (FileNotFoundError, pd.errors.EmptyDataError) as e:
        st.warning(f"No existing fluorophore data found: {e}")
        return pd.DataFrame(columns=DEFAULT_COLUMNS)


@st.cache_data
def load_cross_section_data() -> Dict[str, pd.DataFrame]:
    """Load all two-photon cross-section data files."""
    cross_sections: Dict[str, pd.DataFrame] = {}

    if not XSECTION_DIR.exists():
        st.error(f"Cross-section directory not found: {XSECTION_DIR}")
        return cross_sections

    # Define special case configurations with exact header handling
    SPECIAL_CASES = {
        "IntrinsicFluorophores": {
            "skiprows": [0, 1],  # Skip title and separator lines
            "names": ["wavelength", "riboflavin", "folic_acid", "cholecalciferol", "retinol"],
            "description": "Intrinsic fluorophores including riboflavin, folic acid, etc.",
            "delimiter": r"\s+",
            "encoding": "utf-8",
            "decimal": ".",  # Handle scientific notation
            "thousands": None  # No thousands separator
        },
        "NADH-ProteinBound": {
            "skiprows": [0, 1, 2],  # Skip all header lines
            "names": ["wavelength", "gm_mean", "sd", "gm_mdh", "gm_ad"],
            "description": "NADH in different binding states",
            "delimiter": r"\s+",
            "encoding": "utf-8",
            "decimal": ".",
            "thousands": None
        },
        "Fura2": {
            "skiprows": [0, 1, 2],  # Skip header lines
            "names": ["wavelength", "gm_mean_ca", "sd_ca", "gm_mean_free", "sd_free"],
            "description": "Fura-2 with and without calcium",
            "delimiter": r"\s+",
            "encoding": "utf-8",
            "decimal": ".",
            "thousands": None
        }
    }

    for file_path in XSECTION_DIR.glob("*.txt"):
        try:
            fluorophore_name = file_path.stem

            if fluorophore_name in SPECIAL_CASES:
                config = SPECIAL_CASES[fluorophore_name]
                # Read the file with exact configuration
                df = pd.read_csv(
                    file_path,
                    sep=config["delimiter"],
                    skiprows=config["skiprows"],
                    names=config["names"],
                    encoding=config["encoding"],
                    engine='python',
                    decimal=config["decimal"],
                    thousands=config["thousands"]
                )
                # Add metadata
                df.attrs['description'] = config.get('description', '')
            else:
                # Read the file to determine header structure
                with open(file_path, 'r') as f:
                    lines = f.readlines()
                
                # Find where the data starts (after header lines)
                data_start = 0
                for i, line in enumerate(lines):
                    line = line.strip()
                    # Skip empty lines, comment lines, and header lines
                    if (line and 
                        not line.startswith('#') and 
                        not line.startswith('--') and 
                        not line.startswith('nm') and
                        not 'GM' in line and
                        not 'mean' in line and
                        not 'sd' in line and
                        not '@' in line and
                        not 'n=' in line):
                        # Check if this line looks like data (starts with a number)
                        parts = line.split()
                        if parts and parts[0].replace('.', '').replace('-', '').replace('e', '').replace('E', '').isdigit():
                            data_start = i
                            break
                
                # Read data with tab separator, skipping header lines
                logger.debug(f"Reading {file_path.name} with skiprows={data_start}")
                df = pd.read_csv(
                    file_path,
                    sep='\t',
                    engine='python',
                    comment='#',
                    skiprows=data_start
                )
                
                # Check if we got the expected columns
                if len(df.columns) >= 2:
                    # Success - assign column names
                    if len(df.columns) == 3:
                        df.columns = ["wavelength", "cross_section", "std_dev"]
                    else:
                        df.columns = ["wavelength", "cross_section"]
                else:
                    # Try with whitespace separator
                    df = pd.read_csv(
                        file_path,
                        sep=r'\s+',
                        engine='python',
                        comment='#',
                        skiprows=data_start
                    )
                    
                    if len(df.columns) >= 2:
                        if len(df.columns) == 3:
                            df.columns = ["wavelength", "cross_section", "std_dev"]
                        else:
                            df.columns = ["wavelength", "cross_section"]
                    else:
                        logger.error(f"Could not parse {file_path.name} - insufficient columns after header detection")
                        continue

            # Clean and validate data
            df = df.drop_duplicates()

            # Convert all numeric columns to float, handling scientific notation
            for col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

            # Drop any rows where all numeric columns are NaN
            df = df.dropna(how='all', subset=df.columns[1:])  # Keep row if any numeric data exists
            df = df.sort_values('wavelength')

            # Additional validation
            if df.empty or df['wavelength'].isna().all():
                logger.warning(f"No valid data found in {file_path}")
                continue

            # Log the first few rows for debugging
            logger.debug(f"First few rows of {fluorophore_name}:\n{df.head()}")

            cross_sections[fluorophore_name] = df

        except Exception as e:
            st.error(f"Error loading cross-section data for {file_path.name}: {e}")
            logger.error(f"Failed to load {file_path}: {str(e)}", exc_info=True)
            # Add more detailed error information
            try:
                with open(file_path, 'r') as f:
                    first_lines = [f.readline().strip() for _ in range(10)]
                logger.error(f"First 10 lines of {file_path.name}: {first_lines}")
            except Exception as read_error:
                logger.error(f"Could not read file contents for debugging: {read_error}")

    if not cross_sections:
        st.warning("No cross-section data files were loaded successfully")

    return cross_sections




